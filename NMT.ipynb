{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input is a sentence (sequence) in English \n",
    "- Output is the corresponding sequence in German\n",
    "- Encoder Decoder models with a Deep Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Implement attention\n",
    "2. Comment CODE!!\n",
    "\n",
    "TRAINING with BIGGER DATASET (so far trained only with 60 sentence and it was working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import src.text_processing as text_processing\n",
    "import src.dictionary as dictionary\n",
    "import src.neural_network as neural_network\n",
    "\n",
    "# Update python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55000,)\n",
      "['i', 'know']\n",
      "['ich', 'weiß']\n"
     ]
    }
   ],
   "source": [
    "# Read file containing english and german translations\n",
    "data = text_processing.load_doc(\"./dataset/ENG_to_GER.txt\")\n",
    "\n",
    "# Split data into english and german\n",
    "english_sentences, german_sentences = text_processing.prepare_data(data)\n",
    "\n",
    "# Check and print number of sentences from one language to the other\n",
    "assert(len(english_sentences) == len(german_sentences))\n",
    "print(english_sentences.shape)\n",
    "\n",
    "# Example of sentence with translation\n",
    "print(english_sentences[18])\n",
    "print(german_sentences[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset (training + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 44000\n",
      "Validation samples: 11000\n"
     ]
    }
   ],
   "source": [
    "# Split percentage of training and validation\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Count how many samples into training dataset\n",
    "total_dataset = len(english_sentences)\n",
    "train_dataset = int(total_dataset * split_percentage)\n",
    "\n",
    "# Set random seed to have always same training and validation split\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(total_dataset, train_dataset, replace=False)\n",
    "\n",
    "# Get training data for the two languages\n",
    "training_english = english_sentences[train_indices]\n",
    "training_german = german_sentences[train_indices]\n",
    "\n",
    "# Get validation data\n",
    "validation_english = np.delete(english_sentences, train_indices)\n",
    "validation_german = np.delete(german_sentences, train_indices)\n",
    "\n",
    "print(\"Training samples: \" + str(training_english.shape[0]))\n",
    "print(\"Validation samples: \" + str(validation_english.shape[0]))\n",
    "\n",
    "# Reset seed for randomness\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries for the two languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence in English has 11 tokens.\n",
      "Longest sentence in German has 14 tokens.\n",
      "\n",
      "English dictionary size: 7824\n",
      "German dictionary size: 12472\n"
     ]
    }
   ],
   "source": [
    "# Calculate longest sentence in the two languages\n",
    "english_max_length = text_processing.max_length_sentence(training_english)\n",
    "german_max_length = text_processing.max_length_sentence(training_german) + 2  # + 2 because of <START> and <END> the beginning\n",
    "\n",
    "print(\"Longest sentence in English has \" + str(english_max_length) + \" tokens.\")\n",
    "print(\"Longest sentence in German has \" + str(german_max_length) + \" tokens.\")\n",
    "print()\n",
    "\n",
    "# Create dictionaries\n",
    "english_dictionary = dictionary.LanguageDictionary(training_english, english_max_length)\n",
    "german_dictionary = dictionary.LanguageDictionary(training_german, german_max_length)\n",
    "\n",
    "# Calculate size of the dictionaries\n",
    "english_dictionary_size = len(english_dictionary.index_to_word)\n",
    "german_dictionary_size = len(german_dictionary.index_to_word)\n",
    "\n",
    "print(\"English dictionary size: \" + str(english_dictionary_size))\n",
    "print(\"German dictionary size: \" + str(german_dictionary_size))\n",
    "\n",
    "# Save dictionaries\n",
    "text_processing.save_dump(english_dictionary, \"./dumps/eng_dict.pickle\")\n",
    "text_processing.save_dump(german_dictionary, \"./dumps/ger_dict.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sequences for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples : 44000\n",
      "(44000, 11)\n",
      "(44000, 14)\n",
      "Validation samples : 11000\n",
      "(11000, 11)\n",
      "(11000, 14)\n"
     ]
    }
   ],
   "source": [
    "# Prepare sequences of training data\n",
    "train_source_input, train_target_input = text_processing.prepare_sequences(training_english, \n",
    "                                                                       training_german, \n",
    "                                                                       english_dictionary, \n",
    "                                                                       german_dictionary)\n",
    "\n",
    "# Prepare sequences of validation data\n",
    "val_source_input, val_target_input = text_processing.prepare_sequences(validation_english, \n",
    "                                                                       validation_german, \n",
    "                                                                       english_dictionary, \n",
    "                                                                       german_dictionary)\n",
    "\n",
    "# Check if same number of samples\n",
    "assert(len(train_source_input) == len(train_target_input))\n",
    "assert(len(val_source_input) == len(val_target_input))\n",
    "\n",
    "# Print shapes data\n",
    "print(\"Training samples : \" + str(len(train_source_input)))\n",
    "print(train_source_input.shape)\n",
    "print(train_target_input.shape)\n",
    "\n",
    "print(\"Validation samples : \" + str(len(val_source_input)))\n",
    "print(val_source_input.shape)\n",
    "print(val_target_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print sample input data in English, German and next word to be predicted in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0  0  0  0  0 37 38 39 40 41]\n",
      "[ 1 32 33 34 35 36  2  0  0  0  0  0  0  0]\n",
      "SOURCE => <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> tom never tells me anything\n",
      "TARGET => <START> tom erzählt mir nie etwas <END> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "sample_sentence_index = 7\n",
    "print(train_source_input[sample_sentence_index])\n",
    "print(train_target_input[sample_sentence_index])\n",
    "\n",
    "print(\"SOURCE => \" + english_dictionary.indices_to_text(train_source_input[sample_sentence_index]))\n",
    "print(\"TARGET => \" + german_dictionary.indices_to_text(train_target_input[sample_sentence_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 128\n",
    "embedding_size = 128\n",
    "lstm_hidden_units = 128\n",
    "lr = 1e-3\n",
    "depth_lstm_bidirectional_layers = 2\n",
    "keep_dropout_prob = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Seq2seq neural network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [None, 11]\n",
      "Encoder embedding: [None, 11, 128]\n",
      "Encoder FW last_state: [None, 128]\n",
      "Encoder BW last_state: [None, 128]\n",
      "Decoder output: [None, None, 256]\n",
      "Logits: [None, None, 12472]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "input_sequence = tf.placeholder(tf.int32, (None, english_dictionary.max_length_sentence), 'inputs')\n",
    "output_sequence = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "target_labels = tf.placeholder(tf.int32, (None, None), 'targets')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Create graph for the network\n",
    "logits = neural_network.create_network(input_sequence, \n",
    "                                       output_sequence, \n",
    "                                       input_keep_prob,\n",
    "                                       output_keep_prob,\n",
    "                                       english_dictionary_size, \n",
    "                                       german_dictionary_size, \n",
    "                                       embedding_size,\n",
    "                                       lstm_hidden_units,\n",
    "                                       depth_lstm_bidirectional_layers,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the loss function, optimizer and other useful tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=target_labels)\n",
    "loss = tf.reduce_mean(ce)\n",
    "\n",
    "# Using Adam (Adaptive learning rate + momentum) for the update of the weights of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=2))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, target_labels))\n",
    "accuracy = tf.reduce_mean(tf.reduce_mean(tf.cast(correct_mask, tf.float32), axis=1))\n",
    "\n",
    "\n",
    "\n",
    "# MODIFY TENSORS FOR ACCURACY in case of teaching forcing methods\n",
    "#correct_mask_int = tf.equal(predictions[:,-1], target_labels[:,-1])\n",
    "#correct_mask = tf.to_float(tf.equal(predictions[:,-1], target_labels[:,-1]))\n",
    "#accuracy = tf.reduce_mean(tf.cast(correct_mask, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    TEACHER FORCING ONLY 1 target output\\n\\n    decoder_input = np.expand_dims(target_sentences[:, j], 1) \\n    decoder_output = np.expand_dims(target_sentences[:, j+1], 1) \\n\\n    encoder_input = [source_sentences[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\\n    decoder_input = [decoder_input[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\\n    decoder_output = [decoder_output[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\\n\\n    assert(len(encoder_input) == len(decoder_input) == len(decoder_output))\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "    If NOT using teacher forcing method:\n",
    "        - train_target_input[start_index:end_index, :-1],\n",
    "        - target_labels: train_target_input[start_index:end_index, 1:],\n",
    "'''\n",
    "\n",
    "'''\n",
    "    TEACHER FORCING ONLY 1 target output\n",
    "\n",
    "    decoder_input = np.expand_dims(target_sentences[:, j], 1) \n",
    "    decoder_output = np.expand_dims(target_sentences[:, j+1], 1) \n",
    "\n",
    "    encoder_input = [source_sentences[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\n",
    "    decoder_input = [decoder_input[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\n",
    "    decoder_output = [decoder_output[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\n",
    "\n",
    "    assert(len(encoder_input) == len(decoder_input) == len(decoder_output))\n",
    "'''       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teacher_forcing_method(session, fetches, source_sentences, target_sentences, target_sentence_length, \n",
    "                           dropout_prob, tensor_input, tensor_output, tensor_label, tensor_input_prob,\n",
    "                           tensor_output_prob):\n",
    "    \n",
    "    accuracies, losses = [], []\n",
    "    total_samples = 0\n",
    "    for j in range(target_sentence_length - 1):\n",
    "\n",
    "        decoder_input = target_sentences[:, :j+1]\n",
    "        decoder_output = target_sentences[:, j+1:j+2]        \n",
    "        \n",
    "        #I do not want to predict anything after <END> => choose where next target char is != \"<PAD>\" (0)\n",
    "        encoder_input = [source_sentences[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\n",
    "        decoder_input = [decoder_input[i] for i in range(len(decoder_output)) if decoder_output[i] != 0]\n",
    "        decoder_output = [target_sentences[i, 1:j+2] for i in range(len(decoder_output)) if decoder_output[i] != 0]\n",
    "\n",
    "        assert(len(encoder_input) == len(decoder_input) == len(decoder_output))\n",
    "    \n",
    "        # There could be only sentences shorter than the max => break\n",
    "        if len(encoder_input) == 0:\n",
    "            break\n",
    "            \n",
    "        #print([english_dictionary.indices_to_text(tmp) for tmp in encoder_input])\n",
    "        #print([german_dictionary.indices_to_text(tmp) for tmp in decoder_input])\n",
    "        #print([german_dictionary.indices_to_text(tmp) for tmp in decoder_output])\n",
    "        #print()\n",
    "        \n",
    "        # Run TF graph\n",
    "        _, value_accuracy, value_loss = sess.run(fetches, feed_dict={\n",
    "                                                tensor_input: encoder_input,\n",
    "                                                tensor_output: decoder_input,\n",
    "                                                tensor_label: decoder_output,\n",
    "                                                tensor_input_prob: dropout_prob,\n",
    "                                                tensor_output_prob: dropout_prob })\n",
    "        # Weighted avg of accuracy\n",
    "        total_samples += len(decoder_output)\n",
    "        accuracies.append(value_accuracy * float(len(decoder_output)))\n",
    "        losses.append(value_loss * len(decoder_output))\n",
    "    \n",
    "    accuracies = np.array(accuracies)\n",
    "    losses = np.array(losses)\n",
    "\n",
    "    return np.sum(accuracies) / float(total_samples), np.sum(losses) / float(total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations per epoch: 343\n",
      "VALIDATION loss: 9.314646, accuracy: 0.5230981\n",
      "Loss: 2.2136345, Accuracy: 0.672476\n",
      "Training epoch: 1, AVG loss: 2.701314, AVG accuracy: 0.6385796\n",
      "\n",
      "VALIDATION loss: 2.2098331, accuracy: 0.6856618\n",
      "Loss: 1.9288319, Accuracy: 0.719351\n",
      "Training epoch: 2, AVG loss: 2.0662255, AVG accuracy: 0.70159847\n",
      "\n",
      "VALIDATION loss: 1.9339939, accuracy: 0.720567\n",
      "Loss: 1.7377322, Accuracy: 0.75120187\n",
      "Training epoch: 3, AVG loss: 1.8388755, AVG accuracy: 0.72286934\n",
      "\n",
      "VALIDATION loss: 1.7624063, accuracy: 0.73698384\n",
      "Loss: 1.6116804, Accuracy: 0.747596\n",
      "Training epoch: 4, AVG loss: 1.6769046, AVG accuracy: 0.73687994\n",
      "\n",
      "VALIDATION loss: 1.6370198, accuracy: 0.75038874\n",
      "Loss: 1.5861386, Accuracy: 0.7487981\n",
      "Training epoch: 5, AVG loss: 1.54486, AVG accuracy: 0.74928784\n",
      "\n",
      "VALIDATION loss: 1.5362573, accuracy: 0.761489\n",
      "Loss: 1.3744733, Accuracy: 0.77463937\n",
      "Training epoch: 6, AVG loss: 1.4338238, AVG accuracy: 0.7587253\n",
      "\n",
      "VALIDATION loss: 1.4516664, accuracy: 0.7708781\n",
      "Loss: 1.3545127, Accuracy: 0.7614182\n",
      "Training epoch: 7, AVG loss: 1.3324579, AVG accuracy: 0.7676726\n",
      "\n",
      "VALIDATION loss: 1.3801239, accuracy: 0.7793762\n",
      "Loss: 1.1392428, Accuracy: 0.7920673\n",
      "Training epoch: 8, AVG loss: 1.2434093, AVG accuracy: 0.7765519\n",
      "\n",
      "VALIDATION loss: 1.3177235, accuracy: 0.7872101\n",
      "Loss: 1.1750455, Accuracy: 0.7836538\n",
      "Training epoch: 9, AVG loss: 1.1608938, AVG accuracy: 0.78474665\n",
      "\n",
      "VALIDATION loss: 1.2654786, accuracy: 0.7953124\n",
      "Loss: 1.0605915, Accuracy: 0.8028846\n",
      "Training epoch: 10, AVG loss: 1.0846573, AVG accuracy: 0.79295105\n",
      "\n",
      "VALIDATION loss: 1.2196345, accuracy: 0.80180275\n",
      "Loss: 1.0302217, Accuracy: 0.8022836\n",
      "Training epoch: 11, AVG loss: 1.016411, AVG accuracy: 0.80059034\n",
      "\n",
      "VALIDATION loss: 1.1831902, accuracy: 0.80693585\n",
      "Loss: 0.870642, Accuracy: 0.8281249\n",
      "Training epoch: 12, AVG loss: 0.9538441, AVG accuracy: 0.80781215\n",
      "\n",
      "VALIDATION loss: 1.1488148, accuracy: 0.8132351\n",
      "Loss: 0.96596783, Accuracy: 0.8064903\n",
      "Training epoch: 13, AVG loss: 0.8981452, AVG accuracy: 0.81453276\n",
      "\n",
      "VALIDATION loss: 1.1216669, accuracy: 0.8181278\n",
      "Loss: 0.7931353, Accuracy: 0.8299278\n",
      "Training epoch: 14, AVG loss: 0.8470907, AVG accuracy: 0.821639\n",
      "\n",
      "VALIDATION loss: 1.1006584, accuracy: 0.82149315\n",
      "Loss: 0.75730133, Accuracy: 0.8473556\n",
      "Training epoch: 15, AVG loss: 0.79973394, AVG accuracy: 0.8288177\n",
      "\n",
      "VALIDATION loss: 1.0788676, accuracy: 0.8255232\n",
      "Loss: 0.76630825, Accuracy: 0.841346\n",
      "Training epoch: 16, AVG loss: 0.7571431, AVG accuracy: 0.836006\n",
      "\n",
      "VALIDATION loss: 1.0625452, accuracy: 0.828938\n",
      "Loss: 0.74299496, Accuracy: 0.82451916\n",
      "Training epoch: 17, AVG loss: 0.71910053, AVG accuracy: 0.8417362\n",
      "\n",
      "VALIDATION loss: 1.0469058, accuracy: 0.8317871\n",
      "Loss: 0.74346566, Accuracy: 0.8341345\n",
      "Training epoch: 18, AVG loss: 0.6818133, AVG accuracy: 0.8486633\n",
      "\n",
      "VALIDATION loss: 1.0345591, accuracy: 0.83499\n",
      "Loss: 0.6888856, Accuracy: 0.83894217\n",
      "Training epoch: 19, AVG loss: 0.6491777, AVG accuracy: 0.85392267\n",
      "\n",
      "VALIDATION loss: 1.0255557, accuracy: 0.8368211\n",
      "Loss: 0.64687717, Accuracy: 0.85456717\n",
      "Training epoch: 20, AVG loss: 0.61928165, AVG accuracy: 0.8595428\n",
      "\n",
      "VALIDATION loss: 1.0146081, accuracy: 0.8392604\n",
      "Loss: 0.6227472, Accuracy: 0.85456717\n",
      "Training epoch: 21, AVG loss: 0.59289026, AVG accuracy: 0.8648774\n",
      "\n",
      "VALIDATION loss: 1.0052158, accuracy: 0.84107035\n",
      "Loss: 0.60037357, Accuracy: 0.86057675\n",
      "Training epoch: 22, AVG loss: 0.566401, AVG accuracy: 0.8697524\n",
      "\n",
      "VALIDATION loss: 1.0005649, accuracy: 0.842442\n",
      "Loss: 0.5529409, Accuracy: 0.8665864\n",
      "Training epoch: 23, AVG loss: 0.5437066, AVG accuracy: 0.874275\n",
      "\n",
      "VALIDATION loss: 0.9913499, accuracy: 0.8445559\n",
      "Loss: 0.5223027, Accuracy: 0.8749999\n",
      "Training epoch: 24, AVG loss: 0.52403617, AVG accuracy: 0.87815106\n",
      "\n",
      "VALIDATION loss: 0.9870682, accuracy: 0.8466981\n",
      "Loss: 0.6132988, Accuracy: 0.86057687\n",
      "Training epoch: 25, AVG loss: 0.5032527, AVG accuracy: 0.8819815\n",
      "\n",
      "VALIDATION loss: 0.9840106, accuracy: 0.84785765\n",
      "Loss: 0.48424524, Accuracy: 0.8834134\n",
      "Training epoch: 26, AVG loss: 0.48530543, AVG accuracy: 0.8855952\n",
      "\n",
      "VALIDATION loss: 0.9787173, accuracy: 0.8491232\n",
      "Loss: 0.41890332, Accuracy: 0.89783645\n",
      "Training epoch: 27, AVG loss: 0.46700853, AVG accuracy: 0.88893914\n",
      "\n",
      "VALIDATION loss: 0.97966397, accuracy: 0.8498725\n",
      "Loss: 0.50608885, Accuracy: 0.8804085\n",
      "Training epoch: 28, AVG loss: 0.45173535, AVG accuracy: 0.8921787\n",
      "\n",
      "VALIDATION loss: 0.9775784, accuracy: 0.8513008\n",
      "Loss: 0.41889, Accuracy: 0.89182675\n",
      "Training epoch: 29, AVG loss: 0.43786097, AVG accuracy: 0.8946457\n",
      "\n",
      "VALIDATION loss: 0.9755783, accuracy: 0.85193706\n",
      "Loss: 0.4884865, Accuracy: 0.88581717\n",
      "Training epoch: 30, AVG loss: 0.42397672, AVG accuracy: 0.89789474\n",
      "\n",
      "VALIDATION loss: 0.9747563, accuracy: 0.8528137\n",
      "Loss: 0.37702766, Accuracy: 0.9032451\n",
      "Training epoch: 31, AVG loss: 0.41168573, AVG accuracy: 0.90029985\n",
      "\n",
      "VALIDATION loss: 0.9742904, accuracy: 0.8531601\n",
      "Loss: 0.4606668, Accuracy: 0.8960335\n",
      "Training epoch: 32, AVG loss: 0.39873627, AVG accuracy: 0.9024489\n",
      "\n",
      "VALIDATION loss: 0.9776665, accuracy: 0.8539521\n",
      "Loss: 0.384352, Accuracy: 0.90084124\n",
      "Training epoch: 33, AVG loss: 0.39008808, AVG accuracy: 0.9037907\n",
      "\n",
      "VALIDATION loss: 0.9743132, accuracy: 0.85520345\n",
      "Loss: 0.36540577, Accuracy: 0.9044471\n",
      "Training epoch: 34, AVG loss: 0.37810862, AVG accuracy: 0.90660125\n",
      "\n",
      "VALIDATION loss: 0.97544473, accuracy: 0.85580444\n",
      "Loss: 0.42140865, Accuracy: 0.9104566\n",
      "Training epoch: 35, AVG loss: 0.36897415, AVG accuracy: 0.9082804\n",
      "\n",
      "VALIDATION loss: 0.97727287, accuracy: 0.8565397\n",
      "Loss: 0.39835057, Accuracy: 0.90264404\n",
      "Training epoch: 36, AVG loss: 0.3591708, AVG accuracy: 0.9107104\n",
      "\n",
      "VALIDATION loss: 0.97949517, accuracy: 0.85663885\n",
      "Loss: 0.3285293, Accuracy: 0.9134615\n",
      "Training epoch: 37, AVG loss: 0.34886378, AVG accuracy: 0.91279864\n",
      "\n",
      "VALIDATION loss: 0.98058635, accuracy: 0.85715485\n",
      "Loss: 0.32664123, Accuracy: 0.9236778\n",
      "Training epoch: 38, AVG loss: 0.34203595, AVG accuracy: 0.9138924\n",
      "\n",
      "VALIDATION loss: 0.98009795, accuracy: 0.85797507\n",
      "Loss: 0.3631894, Accuracy: 0.90865374\n",
      "Training epoch: 39, AVG loss: 0.33387294, AVG accuracy: 0.9154015\n",
      "\n",
      "VALIDATION loss: 0.97913027, accuracy: 0.8583356\n",
      "Loss: 0.3493469, Accuracy: 0.919471\n",
      "Training epoch: 40, AVG loss: 0.32778612, AVG accuracy: 0.9166066\n",
      "\n",
      "Loss: 0.34882185, Accuracy: 0.91225946\n",
      "Training epoch: 41, AVG loss: 0.31928083, AVG accuracy: 0.9178476\n",
      "\n",
      "VALIDATION loss: 0.987692, accuracy: 0.8589648\n",
      "Loss: 0.29762968, Accuracy: 0.9212739\n",
      "Training epoch: 42, AVG loss: 0.31266877, AVG accuracy: 0.91975886\n",
      "\n",
      "VALIDATION loss: 0.9877589, accuracy: 0.86010313\n",
      "Loss: 0.3243449, Accuracy: 0.9122596\n",
      "Training epoch: 43, AVG loss: 0.30489722, AVG accuracy: 0.9211435\n",
      "\n",
      "Loss: 0.30185658, Accuracy: 0.9224758\n",
      "Training epoch: 44, AVG loss: 0.30067715, AVG accuracy: 0.9218909\n",
      "\n",
      "Loss: 0.3436286, Accuracy: 0.9146634\n",
      "Training epoch: 45, AVG loss: 0.29351124, AVG accuracy: 0.92369306\n",
      "\n",
      "VALIDATION loss: 0.9916126, accuracy: 0.860301\n",
      "Loss: 0.2677886, Accuracy: 0.9290864\n",
      "Training epoch: 46, AVG loss: 0.28795385, AVG accuracy: 0.92487335\n",
      "\n",
      "Loss: 0.28966758, Accuracy: 0.9266826\n",
      "Training epoch: 47, AVG loss: 0.28188622, AVG accuracy: 0.9263602\n",
      "\n",
      "Loss: 0.29072645, Accuracy: 0.92608166\n",
      "Training epoch: 48, AVG loss: 0.27686357, AVG accuracy: 0.9270933\n",
      "\n",
      "VALIDATION loss: 0.9975818, accuracy: 0.86111414\n",
      "Loss: 0.29958424, Accuracy: 0.92247593\n",
      "Training epoch: 49, AVG loss: 0.27189764, AVG accuracy: 0.9282522\n",
      "\n",
      "VALIDATION loss: 0.99970955, accuracy: 0.8612838\n",
      "Loss: 0.2563219, Accuracy: 0.9314903\n",
      "Training epoch: 50, AVG loss: 0.26771703, AVG accuracy: 0.92883664\n",
      "\n",
      "Loss: 0.26638275, Accuracy: 0.9284855\n",
      "Training epoch: 51, AVG loss: 0.2620039, AVG accuracy: 0.93028086\n",
      "\n",
      "VALIDATION loss: 1.0061052, accuracy: 0.86170805\n",
      "Loss: 0.30673203, Accuracy: 0.9200721\n",
      "Training epoch: 52, AVG loss: 0.25707757, AVG accuracy: 0.93163145\n",
      "\n",
      "Loss: 0.28544965, Accuracy: 0.9224758\n",
      "Training epoch: 53, AVG loss: 0.25355035, AVG accuracy: 0.93218595\n",
      "\n",
      "VALIDATION loss: 1.0072997, accuracy: 0.86193424\n",
      "Loss: 0.24738511, Accuracy: 0.9296874\n",
      "Training epoch: 54, AVG loss: 0.24969634, AVG accuracy: 0.932766\n",
      "\n",
      "Loss: 0.28405717, Accuracy: 0.9266826\n",
      "Training epoch: 55, AVG loss: 0.24502356, AVG accuracy: 0.9337512\n",
      "\n",
      "VALIDATION loss: 1.0137048, accuracy: 0.8622878\n",
      "Loss: 0.25463438, Accuracy: 0.9314903\n",
      "Training epoch: 56, AVG loss: 0.24072395, AVG accuracy: 0.9349414\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.23649451, Accuracy: 0.9344951\n",
      "Training epoch: 57, AVG loss: 0.23659465, AVG accuracy: 0.935457\n",
      "\n",
      "Loss: 0.25125268, Accuracy: 0.92788446\n",
      "Training epoch: 58, AVG loss: 0.2338282, AVG accuracy: 0.936101\n",
      "\n",
      "VALIDATION loss: 1.0240797, accuracy: 0.86271894\n",
      "Loss: 0.21116418, Accuracy: 0.9381009\n",
      "Training epoch: 59, AVG loss: 0.23000114, AVG accuracy: 0.9369295\n",
      "\n",
      "Loss: 0.21366031, Accuracy: 0.9381009\n",
      "Training epoch: 60, AVG loss: 0.22691005, AVG accuracy: 0.9377444\n",
      "\n",
      "VALIDATION loss: 1.0246682, accuracy: 0.8630231\n",
      "Loss: 0.2003677, Accuracy: 0.9441105\n",
      "Training epoch: 61, AVG loss: 0.22286798, AVG accuracy: 0.9390447\n",
      "\n",
      "VALIDATION loss: 1.0303345, accuracy: 0.8634968\n",
      "Loss: 0.19871448, Accuracy: 0.9435095\n",
      "Training epoch: 62, AVG loss: 0.21930826, AVG accuracy: 0.93951577\n",
      "\n",
      "VALIDATION loss: 1.0281079, accuracy: 0.8637584\n",
      "Loss: 0.19946466, Accuracy: 0.9435095\n",
      "Training epoch: 63, AVG loss: 0.21640784, AVG accuracy: 0.94032204\n",
      "\n",
      "Loss: 0.19614601, Accuracy: 0.9435095\n",
      "Training epoch: 64, AVG loss: 0.21312124, AVG accuracy: 0.94097847\n",
      "\n",
      "Loss: 0.21156599, Accuracy: 0.9399038\n",
      "Training epoch: 65, AVG loss: 0.21014021, AVG accuracy: 0.9416842\n",
      "\n",
      "Loss: 0.21248896, Accuracy: 0.9423076\n",
      "Training epoch: 66, AVG loss: 0.20809206, AVG accuracy: 0.9419625\n",
      "\n",
      "VALIDATION loss: 1.0346119, accuracy: 0.8639564\n",
      "Loss: 0.20066926, Accuracy: 0.9471153\n",
      "Training epoch: 67, AVG loss: 0.20412536, AVG accuracy: 0.9432599\n",
      "\n",
      "Loss: 0.19887654, Accuracy: 0.9471153\n",
      "Training epoch: 68, AVG loss: 0.2029553, AVG accuracy: 0.942866\n",
      "\n",
      "Loss: 0.2108334, Accuracy: 0.9374999\n",
      "Training epoch: 69, AVG loss: 0.20064765, AVG accuracy: 0.94371927\n",
      "\n",
      "Loss: 0.22111799, Accuracy: 0.9344951\n",
      "Training epoch: 70, AVG loss: 0.19652887, AVG accuracy: 0.94454974\n",
      "\n",
      "VALIDATION loss: 1.0478978, accuracy: 0.86421084\n",
      "Loss: 0.1790954, Accuracy: 0.9489181\n",
      "Training epoch: 71, AVG loss: 0.19579495, AVG accuracy: 0.9450151\n",
      "\n",
      "Loss: 0.25928518, Accuracy: 0.93749994\n",
      "Training epoch: 72, AVG loss: 0.19198953, AVG accuracy: 0.94595265\n",
      "\n",
      "Loss: 0.17366567, Accuracy: 0.9555288\n",
      "Training epoch: 73, AVG loss: 0.19058283, AVG accuracy: 0.94619757\n",
      "\n",
      "VALIDATION loss: 1.0581423, accuracy: 0.8643946\n",
      "Loss: 0.17015323, Accuracy: 0.95552874\n",
      "Training epoch: 74, AVG loss: 0.1879002, AVG accuracy: 0.9466332\n",
      "\n",
      "Loss: 0.19839247, Accuracy: 0.9501201\n",
      "Training epoch: 75, AVG loss: 0.18594965, AVG accuracy: 0.9472727\n",
      "\n",
      "Loss: 0.21619463, Accuracy: 0.9423076\n",
      "Training epoch: 76, AVG loss: 0.18320985, AVG accuracy: 0.94770646\n",
      "\n",
      "Loss: 0.18539222, Accuracy: 0.9429085\n",
      "Training epoch: 77, AVG loss: 0.18072428, AVG accuracy: 0.9483375\n",
      "\n",
      "Loss: 0.18295066, Accuracy: 0.9411057\n",
      "Training epoch: 78, AVG loss: 0.17844653, AVG accuracy: 0.9486319\n",
      "\n",
      "VALIDATION loss: 1.072276, accuracy: 0.86445135\n",
      "Loss: 0.17597276, Accuracy: 0.951923\n",
      "Training epoch: 79, AVG loss: 0.176075, AVG accuracy: 0.9494403\n",
      "\n",
      "VALIDATION loss: 1.0693781, accuracy: 0.8648543\n",
      "Loss: 0.15921909, Accuracy: 0.951322\n",
      "Training epoch: 80, AVG loss: 0.17501633, AVG accuracy: 0.949605\n",
      "\n",
      "Loss: 0.18928565, Accuracy: 0.94891816\n",
      "Training epoch: 81, AVG loss: 0.1731237, AVG accuracy: 0.95026153\n",
      "\n",
      "VALIDATION loss: 1.0745896, accuracy: 0.864918\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-83c7ab495ca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m                                             \u001b[0mtarget_labels\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_target_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                                             \u001b[0minput_keep_prob\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkeep_dropout_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                                             output_keep_prob: keep_dropout_prob })\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0;31m# Add values for this mini-batch iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training data variables\n",
    "iterations_training = max((len(train_source_input) // batch_size), 1)\n",
    "print(\"Training iterations per epoch: \" + str(iterations_training))\n",
    "training_overfit = False\n",
    "consecutive_validation_without_saving = 0\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "iterations_validation = max((len(val_source_input) // batch_size), 1)\n",
    "\n",
    "# Initializer for variables in the graph\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Perform each epoch, shuffle training dataset\n",
    "indices = list(range(len(train_source_input)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables in the graph\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle data (with random seed for debug) to not train the network always with the same order\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        train_source_input = train_source_input[indices]\n",
    "        train_target_input = train_target_input[indices]\n",
    "        \n",
    "        # Vector accumulating accuracy and loss during for one epoch\n",
    "        total_accuracies, total_losses = [], []\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for j in range(iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size \n",
    "            \n",
    "            # If last batch, take also elements that are less than batch_size\n",
    "            if j == (iterations_training - 1):\n",
    "                end_index += (batch_size - 1)\n",
    "                \n",
    "            '''avg_accuracy, avg_loss = teacher_forcing_method(sess, [optimizer, accuracy, loss], \n",
    "                                                            train_source_input[start_index:end_index], \n",
    "                                                            train_target_input[start_index:end_index], \n",
    "                                                            german_dictionary.max_length_sentence, keep_dropout_prob,\n",
    "                                                            input_sequence, output_sequence, target_labels, \n",
    "                                                            input_keep_prob, output_keep_prob)'''\n",
    "\n",
    "            _, avg_accuracy, avg_loss = sess.run([optimizer, accuracy, loss], feed_dict={\n",
    "                                            input_sequence: train_source_input[start_index:end_index],\n",
    "                                            output_sequence: train_target_input[start_index:end_index, :-1],\n",
    "                                            target_labels: train_target_input[start_index:end_index, 1:],\n",
    "                                            input_keep_prob: keep_dropout_prob,\n",
    "                                            output_keep_prob: keep_dropout_prob })\n",
    "            \n",
    "            # Add values for this mini-batch iterations\n",
    "            total_losses.append(avg_loss) \n",
    "            total_accuracies.append(avg_accuracy)\n",
    "\n",
    "            # Print loss and accuracy\n",
    "            if (j + 1) % 250 == 0:\n",
    "                print(\"Loss: \" + str(avg_loss) + \", Accuracy: \" + str(avg_accuracy))\n",
    "                \n",
    "            # Statistics on validation set\n",
    "            if (j) % 1000 == 0:\n",
    "                \n",
    "                # Accumulate validation statistics\n",
    "                val_accuracies, val_losses = [], []\n",
    "\n",
    "                # Iterate over mini-batches\n",
    "                for k in range(iterations_validation):\n",
    "                    start_index = k * batch_size\n",
    "                    end_index = (k + 1) * batch_size \n",
    "                    \n",
    "                    if j == (iterations_validation - 1):\n",
    "                        end_index += (batch_size - 1)\n",
    "\n",
    "                    '''avg_accuracy, avg_loss = teacher_forcing_method(sess, [scores, accuracy, loss], \n",
    "                                                                    val_source_input[start_index:end_index], \n",
    "                                                                    val_target_input[start_index:end_index], \n",
    "                                                                    german_dictionary.max_length_sentence, 1.0,\n",
    "                                                                    input_sequence, output_sequence, target_labels, \n",
    "                                                                    input_keep_prob, output_keep_prob)'''\n",
    "                    \n",
    "                    avg_accuracy, avg_loss = sess.run([accuracy, loss], feed_dict={\n",
    "                                            input_sequence: val_source_input[start_index:end_index],\n",
    "                                            output_sequence: val_target_input[start_index:end_index, :-1],\n",
    "                                            target_labels: val_target_input[start_index:end_index, 1:],\n",
    "                                            input_keep_prob: 1.0,\n",
    "                                            output_keep_prob: 1.0 })\n",
    "                    \n",
    "                    # Statistics over the mini-batch\n",
    "                    val_losses.append(avg_loss) \n",
    "                    val_accuracies.append(avg_accuracy)\n",
    "            \n",
    "                # Average validation accuracy over batches\n",
    "                final_val_accuracy = np.mean(val_accuracies)\n",
    "                \n",
    "                # Save model if validation accuracy better\n",
    "                if final_val_accuracy > max_val_acc:\n",
    "                    consecutive_validation_without_saving = 0\n",
    "                    max_val_acc = final_val_accuracy\n",
    "                    print(\"VALIDATION loss: \" + str(np.mean(val_losses)) + \", accuracy: \" + str(final_val_accuracy))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")\n",
    "                else:\n",
    "                    # Count every time check validation accuracy\n",
    "                    consecutive_validation_without_saving += 1\n",
    "                \n",
    "                # If checked validation time many consecutive times without having improvement in accuracy\n",
    "                if consecutive_validation_without_saving >= 8:\n",
    "                    training_overfit = True\n",
    "            \n",
    "        # Epoch statistics\n",
    "        print(\"Training epoch: \" + str(i+1) + \", AVG loss: \" + str(np.mean(np.array(total_losses))) + \n",
    "              \", AVG accuracy: \" + str(np.mean(np.array(total_accuracies))) + \"\\n\")\n",
    "        \n",
    "        if training_overfit:\n",
    "            print(\"Early stopping training because it starts overfitting\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([english_dictionary.indices_to_text(tmp) for tmp in train_source_input[1:4]])\n",
    "print()\n",
    "print([german_dictionary.indices_to_text(tmp) for tmp in train_target_input[1:4, :-1]])\n",
    "print()\n",
    "print([german_dictionary.indices_to_text(tmp) for tmp in train_target_input[1:4, 1:]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild graph quickly if want to run only this part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters network\n",
    "#embedding_size = 300\n",
    "#lstm_neurons = 256\n",
    "\n",
    "# Load dictionaries from pickle\n",
    "english_dictionary = text_processing.load_dump(\"./dumps/eng_dict.pickle\")\n",
    "german_dictionary = text_processing.load_dump(\"./dumps/ger_dict.pickle\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "input_sequence = tf.placeholder(tf.int32, (None, english_dictionary.max_length_sentence), 'inputs')\n",
    "output_sequence = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "target_labels = tf.placeholder(tf.int32, (None, None), 'targets')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Create graph for the network\n",
    "logits = neural_network.create_network(input_sequence, \n",
    "                                       output_sequence, \n",
    "                                       input_keep_prob,\n",
    "                                       output_keep_prob,\n",
    "                                       len(english_dictionary.index_to_word), \n",
    "                                       len(german_dictionary.index_to_word), \n",
    "                                       embedding_size,\n",
    "                                       lstm_hidden_units,\n",
    "                                       depth_lstm_bidirectional_layers,\n",
    "                                       verbose=0)\n",
    "# Predictions\n",
    "scores = tf.nn.softmax(logits)\n",
    "#max_score = tf.reduce_max(scores, axis=1)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt\n",
      "<PAD> <PAD> if i had eyes i would look at you => <START> wenn ich schau würde ich dich sehen <END>\n"
     ]
    }
   ],
   "source": [
    "# TF variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\") \n",
    "\n",
    "    test_source_sentence = [\"If I had eyes, I would look at you\"]\n",
    "    #test_source_sentence = validation_english\n",
    "\n",
    "    for source_sentence in test_source_sentence:\n",
    "\n",
    "        # ONLY IF VALIDATION ENGLISH DATASET USED (DEBUG)\n",
    "        #source_sentence = \" \".join(source_sentence)\n",
    "\n",
    "        # Normalize & tokenize (cut if longer than max_length_source)  \n",
    "        source_preprocessed = text_processing.preprocess_sentence(source_sentence)\n",
    "        \n",
    "        # Convert to numbers\n",
    "        source_encoded = english_dictionary.text_to_indices(source_preprocessed)\n",
    "        \n",
    "        # Add padding\n",
    "        source_input = text_processing.pad_sentence(source_encoded, english_dictionary.max_length_sentence)\n",
    "        #print(english_dictionary.indices_to_text(source_input))\n",
    "        \n",
    "        # Starting target sentence in German\n",
    "        target_sentence = [[\"<START>\"]]\n",
    "        target_encoded = german_dictionary.text_to_indices(target_sentence[0])\n",
    "\n",
    "        i = 0\n",
    "        word_predicted = 0\n",
    "        while word_predicted != 2: # If <END> (index 2), stop\n",
    "\n",
    "            # Perform prediction\n",
    "            pred = sess.run(predictions, feed_dict={input_sequence: [source_input], \n",
    "                                                    output_sequence: [target_encoded],\n",
    "                                                    input_keep_prob: 1.0,\n",
    "                                                    output_keep_prob: 1.0 })\n",
    "            \n",
    "            # Accumulate\n",
    "            target_encoded.append(pred[0][i])\n",
    "            word_predicted = pred[0][i]\n",
    "            i += 1\n",
    "\n",
    "        print(english_dictionary.indices_to_text(source_input) + \" => \"\n",
    "              + german_dictionary.indices_to_text(target_encoded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
