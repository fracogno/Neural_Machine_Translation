{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input is a sentence (sequence) in English \n",
    "- Output is the corresponding sequence in German\n",
    "- Encoder Decoder models with a Deep Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Implement attention\n",
    "2. Comment CODE!!\n",
    "\n",
    "TRAINING with BIGGER DATASET (so far trained only with 60 sentence and it was working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import src.text_processing as text_processing\n",
    "import src.dictionary as dictionary\n",
    "import src.neural_network as neural_network\n",
    "\n",
    "# Update python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70,)\n",
      "['be', 'nice']\n",
      "['seien', 'sie', 'nett']\n"
     ]
    }
   ],
   "source": [
    "# Read file containing english and german translations\n",
    "data = text_processing.load_doc(\"./dataset/ENG_to_GER.txt\")\n",
    "\n",
    "# Split data into english and german\n",
    "english_sentences, german_sentences = text_processing.prepare_data(data)\n",
    "\n",
    "# Check and print number of sentences from one language to the other\n",
    "assert(len(english_sentences) == len(german_sentences))\n",
    "print(english_sentences.shape)\n",
    "\n",
    "# Example of sentence with translation\n",
    "print(english_sentences[55])\n",
    "print(german_sentences[55])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset (training + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 56\n",
      "Validation samples: 14\n"
     ]
    }
   ],
   "source": [
    "# Split percentage of training and validation\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Count how many samples into training dataset\n",
    "total_dataset = len(english_sentences)\n",
    "train_dataset = int(total_dataset * split_percentage)\n",
    "\n",
    "# Set random seed to have always same training and validation split\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(total_dataset, train_dataset, replace=False)\n",
    "\n",
    "# Get training data for the two languages\n",
    "training_english = english_sentences[train_indices]\n",
    "training_german = german_sentences[train_indices]\n",
    "\n",
    "# Get validation data\n",
    "validation_english = np.delete(english_sentences, train_indices)\n",
    "validation_german = np.delete(german_sentences, train_indices)\n",
    "\n",
    "print(\"Training samples: \" + str(training_english.shape[0]))\n",
    "print(\"Validation samples: \" + str(validation_english.shape[0]))\n",
    "\n",
    "# Reset seed for randomness\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries for the two languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence in English has 3 tokens.\n",
      "Longest sentence in German has 7 tokens.\n",
      "\n",
      "English dictionary size: 40\n",
      "German dictionary size: 90\n"
     ]
    }
   ],
   "source": [
    "# Calculate longest sentence in the two languages\n",
    "english_max_length = text_processing.max_length_sentence(training_english)\n",
    "german_max_length = text_processing.max_length_sentence(training_german) + 1  # Plus one because I add <START> at the beginning\n",
    "\n",
    "print(\"Longest sentence in English has \" + str(english_max_length) + \" tokens.\")\n",
    "print(\"Longest sentence in German has \" + str(german_max_length) + \" tokens.\")\n",
    "print()\n",
    "\n",
    "# Create dictionaries\n",
    "english_dictionary = dictionary.LanguageDictionary(training_english, english_max_length)\n",
    "german_dictionary = dictionary.LanguageDictionary(training_german, german_max_length)\n",
    "\n",
    "# Calculate size of the dictionaries\n",
    "english_dictionary_size = len(english_dictionary.index_to_word)\n",
    "german_dictionary_size = len(german_dictionary.index_to_word)\n",
    "\n",
    "print(\"English dictionary size: \" + str(english_dictionary_size))\n",
    "print(\"German dictionary size: \" + str(german_dictionary_size))\n",
    "\n",
    "# Save dictionaries\n",
    "text_processing.save_dump(english_dictionary, \"./dumps/eng_dict.pickle\")\n",
    "text_processing.save_dump(german_dictionary, \"./dumps/ger_dict.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sequences for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples : 56\n",
      "(56, 3)\n",
      "(56, 7)\n",
      "Validation samples : 14\n",
      "(14, 3)\n",
      "(14, 7)\n"
     ]
    }
   ],
   "source": [
    "# Prepare sequences of training data\n",
    "train_source_input, train_target_input = text_processing.prepare_sequences(training_english, \n",
    "                                                                       training_german, \n",
    "                                                                       english_dictionary, \n",
    "                                                                       german_dictionary)\n",
    "\n",
    "# Prepare sequences of validation data\n",
    "val_source_input, val_target_input = text_processing.prepare_sequences(validation_english, \n",
    "                                                                       validation_german, \n",
    "                                                                       english_dictionary, \n",
    "                                                                       german_dictionary)\n",
    "\n",
    "# Check if same number of samples\n",
    "assert(len(train_source_input) == len(train_target_input))\n",
    "assert(len(val_source_input) == len(val_target_input))\n",
    "\n",
    "# Print shapes data\n",
    "print(\"Training samples : \" + str(len(train_source_input)))\n",
    "print(train_source_input.shape)\n",
    "print(train_target_input.shape)\n",
    "\n",
    "print(\"Validation samples : \" + str(len(val_source_input)))\n",
    "print(val_source_input.shape)\n",
    "print(val_target_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print sample input data in English, German and next word to be predicted in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  0 14]\n",
      "[1 6 2 0 0 0 0]\n",
      "SOURCE => <PAD> <PAD> hello\n",
      "TARGET => <START> hallo <END> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "sample_sentence_index = 6\n",
    "print(train_source_input[sample_sentence_index])\n",
    "print(train_target_input[sample_sentence_index])\n",
    "\n",
    "print(\"SOURCE => \" + english_dictionary.indices_to_text(train_source_input[sample_sentence_index]))\n",
    "print(\"TARGET => \" + german_dictionary.indices_to_text(train_target_input[sample_sentence_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 64\n",
    "embedding_size = 64\n",
    "lstm_hidden_units = 128\n",
    "lr = 1e-3\n",
    "depth_lstm_bidirectional_layers = 1\n",
    "keep_dropout_prob = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model encoder-decoder with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [None, 3]\n",
      "Encoder embedding: [None, 3, 64]\n",
      "Encoder FW last_state: [None, 128]\n",
      "Encoder BW last_state: [None, 128]\n",
      "Decoder output: [None, None, 256]\n",
      "Logits: [None, None, 90]\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "input_sequence = tf.placeholder(tf.int32, (None, english_dictionary.max_length_sentence), 'inputs')\n",
    "output_sequence = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "target_labels = tf.placeholder(tf.int32, (None, None), 'targets')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Create graph for the network\n",
    "logits = neural_network.create_network(input_sequence, \n",
    "                                       output_sequence, \n",
    "                                       input_keep_prob,\n",
    "                                       output_keep_prob,\n",
    "                                       english_dictionary_size, \n",
    "                                       german_dictionary_size, \n",
    "                                       embedding_size,\n",
    "                                       lstm_hidden_units,\n",
    "                                       depth_lstm_bidirectional_layers,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the loss function, optimizer and other useful tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=target_labels)\n",
    "loss = tf.reduce_mean(ce)\n",
    "\n",
    "# Using Adam (Adaptive learning rate + momentum) for the update of the weights of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=2))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, target_labels))\n",
    "accuracy = tf.reduce_mean(tf.reduce_mean(tf.cast(correct_mask, tf.float32), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Softmax:0\", shape=(?, ?, 90), dtype=float32)\n",
      "Tensor(\"ToInt32:0\", shape=(?, ?), dtype=int32)\n",
      "Tensor(\"ToFloat:0\", shape=(?, ?), dtype=float32)\n",
      "Tensor(\"Mean_2:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(predictions)\n",
    "print(correct_mask)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations per epoch: 1\n",
      "Training loss: 4.4771194, accuracy: 0.020833334\n",
      "VALIDATION loss: 4.3400617, accuracy: 0.32142857\n",
      "Training loss: 4.273194, accuracy: 0.37797618\n",
      "VALIDATION loss: 4.1295815, accuracy: 0.46428576\n",
      "Training loss: 4.057469, accuracy: 0.52678573\n",
      "VALIDATION loss: 3.9003692, accuracy: 0.53571427\n",
      "Training loss: 3.8359578, accuracy: 0.5119048\n",
      "Training loss: 3.559524, accuracy: 0.5\n",
      "Training loss: 3.2796872, accuracy: 0.48511907\n",
      "Training loss: 2.9464107, accuracy: 0.47916666\n",
      "Training loss: 2.637219, accuracy: 0.4761905\n",
      "Training loss: 2.4188752, accuracy: 0.4761905\n",
      "Training loss: 2.3009858, accuracy: 0.4761905\n",
      "Training loss: 2.2858515, accuracy: 0.47619048\n",
      "Training loss: 2.317586, accuracy: 0.47619048\n",
      "Training loss: 2.2756832, accuracy: 0.47619042\n",
      "Training loss: 2.2116084, accuracy: 0.47619048\n",
      "Training loss: 2.0918448, accuracy: 0.49107143\n",
      "Training loss: 2.0160213, accuracy: 0.51488096\n",
      "VALIDATION loss: 1.948818, accuracy: 0.5833334\n",
      "Training loss: 1.9952133, accuracy: 0.5595238\n",
      "VALIDATION loss: 1.9755987, accuracy: 0.5952381\n",
      "Training loss: 1.9751616, accuracy: 0.59226185\n",
      "Training loss: 1.9784358, accuracy: 0.5595238\n",
      "Training loss: 1.9814439, accuracy: 0.55952376\n",
      "Training loss: 1.9369603, accuracy: 0.57142854\n",
      "VALIDATION loss: 1.9992793, accuracy: 0.60714287\n",
      "Training loss: 1.9070208, accuracy: 0.60714287\n",
      "VALIDATION loss: 1.9776303, accuracy: 0.63095236\n",
      "Training loss: 1.8649645, accuracy: 0.6458334\n",
      "VALIDATION loss: 1.9623209, accuracy: 0.64285713\n",
      "Training loss: 1.8243291, accuracy: 0.6547619\n",
      "Training loss: 1.8217083, accuracy: 0.65178573\n",
      "Training loss: 1.7837607, accuracy: 0.6458334\n",
      "Training loss: 1.7826892, accuracy: 0.639881\n",
      "Training loss: 1.7643439, accuracy: 0.6130953\n",
      "Training loss: 1.7511157, accuracy: 0.6160714\n",
      "Training loss: 1.7516954, accuracy: 0.610119\n",
      "Training loss: 1.7199996, accuracy: 0.61011904\n",
      "Training loss: 1.69111, accuracy: 0.610119\n",
      "Training loss: 1.6834333, accuracy: 0.63988096\n",
      "Training loss: 1.6509185, accuracy: 0.639881\n",
      "Training loss: 1.6569887, accuracy: 0.6369047\n",
      "Training loss: 1.6357231, accuracy: 0.6458334\n",
      "VALIDATION loss: 1.9186449, accuracy: 0.6547619\n",
      "Training loss: 1.5952737, accuracy: 0.65476185\n",
      "Training loss: 1.598999, accuracy: 0.65773815\n",
      "Training loss: 1.5566646, accuracy: 0.66071427\n",
      "Training loss: 1.547777, accuracy: 0.66369045\n",
      "Training loss: 1.535013, accuracy: 0.66964287\n",
      "Training loss: 1.4816214, accuracy: 0.66964287\n",
      "Training loss: 1.4994973, accuracy: 0.6666667\n",
      "Training loss: 1.4574447, accuracy: 0.66964287\n",
      "Training loss: 1.4479823, accuracy: 0.6696428\n",
      "Training loss: 1.4324161, accuracy: 0.66964287\n",
      "Training loss: 1.4081188, accuracy: 0.6666666\n",
      "Training loss: 1.3775096, accuracy: 0.672619\n",
      "Training loss: 1.3520583, accuracy: 0.66964287\n",
      "Training loss: 1.3240517, accuracy: 0.6666666\n",
      "Training loss: 1.3082248, accuracy: 0.67857134\n",
      "Training loss: 1.2801771, accuracy: 0.6755952\n",
      "Training loss: 1.2567427, accuracy: 0.68154764\n",
      "Training loss: 1.2545947, accuracy: 0.6755953\n",
      "VALIDATION loss: 1.9195054, accuracy: 0.6785714\n",
      "Training loss: 1.2266645, accuracy: 0.6875\n",
      "VALIDATION loss: 1.9252632, accuracy: 0.69047624\n",
      "Training loss: 1.1983496, accuracy: 0.69345236\n",
      "VALIDATION loss: 1.9314561, accuracy: 0.70238096\n",
      "Training loss: 1.180378, accuracy: 0.70535713\n",
      "Training loss: 1.1407763, accuracy: 0.71428573\n",
      "VALIDATION loss: 1.9461743, accuracy: 0.71428573\n",
      "Training loss: 1.1216639, accuracy: 0.72619045\n",
      "Training loss: 1.0913976, accuracy: 0.7291666\n",
      "Training loss: 1.0822564, accuracy: 0.73214287\n",
      "Training loss: 1.0534976, accuracy: 0.735119\n",
      "VALIDATION loss: 1.9949431, accuracy: 0.72619045\n",
      "Training loss: 1.0239954, accuracy: 0.7291667\n",
      "Training loss: 0.98796374, accuracy: 0.73214287\n",
      "Training loss: 0.9721047, accuracy: 0.7380952\n",
      "Training loss: 0.96982664, accuracy: 0.75\n",
      "Training loss: 0.9217219, accuracy: 0.7410714\n",
      "Training loss: 0.90775996, accuracy: 0.7619047\n",
      "Training loss: 0.8876065, accuracy: 0.7559523\n",
      "Training loss: 0.8623657, accuracy: 0.7619048\n",
      "Training loss: 0.8348921, accuracy: 0.77678573\n",
      "Training loss: 0.8189292, accuracy: 0.77976185\n",
      "Training loss: 0.80435973, accuracy: 0.76785713\n",
      "Training loss: 0.77884775, accuracy: 0.7708334\n",
      "Training loss: 0.73513895, accuracy: 0.8065476\n",
      "Training loss: 0.744245, accuracy: 0.78571427\n",
      "Training loss: 0.71224165, accuracy: 0.8065476\n",
      "VALIDATION loss: 2.2119799, accuracy: 0.7380953\n",
      "Training loss: 0.6956956, accuracy: 0.81547624\n",
      "Training loss: 0.6885506, accuracy: 0.80059516\n",
      "Training loss: 0.6624299, accuracy: 0.80952376\n",
      "Training loss: 0.6435179, accuracy: 0.8244047\n",
      "Training loss: 0.64230907, accuracy: 0.8035714\n",
      "Training loss: 0.62992316, accuracy: 0.8154761\n",
      "Training loss: 0.59033346, accuracy: 0.84821427\n",
      "Training loss: 0.5972536, accuracy: 0.83928573\n",
      "Training loss: 0.5979293, accuracy: 0.83928573\n",
      "Training loss: 0.5637339, accuracy: 0.84226185\n",
      "Training loss: 0.5612864, accuracy: 0.8392857\n",
      "Training loss: 0.5362625, accuracy: 0.86011904\n",
      "Training loss: 0.52074933, accuracy: 0.8809523\n",
      "Training loss: 0.50938034, accuracy: 0.8720237\n",
      "Training loss: 0.49551046, accuracy: 0.87202376\n",
      "Training loss: 0.4732675, accuracy: 0.8779761\n",
      "Training loss: 0.47957605, accuracy: 0.8690476\n",
      "Training loss: 0.47159082, accuracy: 0.8720237\n",
      "Training loss: 0.4483632, accuracy: 0.8779761\n",
      "Training loss: 0.44030404, accuracy: 0.88095236\n",
      "Training loss: 0.43380943, accuracy: 0.88988096\n",
      "Training loss: 0.4269816, accuracy: 0.88095236\n",
      "Training loss: 0.4136491, accuracy: 0.8869047\n"
     ]
    }
   ],
   "source": [
    "# Training data variables\n",
    "num_iterations_training = max(len(train_source_input) // batch_size, 1)\n",
    "print(\"Training iterations per epoch: \" + str(num_iterations_training))\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "val_batch_size = 64\n",
    "num_iterations_validation = max(len(val_source_input) // val_batch_size, 1)\n",
    "\n",
    "# Initializer for variables in the graph\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Perform each epoch, shuffle training dataset\n",
    "indices = list(range(len(train_source_input)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables in the graph\n",
    "    sess.run(init)\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle indices with a random seed\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        \n",
    "        # Shuffle data to not feed the network with always same sequence of data\n",
    "        train_source_input = train_source_input[indices]\n",
    "        train_target_input = train_target_input[indices]\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for j in range(num_iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size\n",
    "\n",
    "            # Forward and backpropagation on training data\n",
    "            _, train_loss, train_acc = sess.run([optimizer, loss, accuracy], feed_dict={\n",
    "                                                            input_sequence: train_source_input[start_index:end_index],\n",
    "                                                            output_sequence: train_target_input[start_index:end_index, :-1],\n",
    "                                                            target_labels: train_target_input[start_index:end_index, 1:],\n",
    "                                                            input_keep_prob: keep_dropout_prob,\n",
    "                                                            output_keep_prob: keep_dropout_prob })\n",
    "            \n",
    "            # Print training loss and accuracy\n",
    "            if j % 250 == 0:\n",
    "                print(\"Training loss: \" + str(train_loss) + \", accuracy: \" + str(train_acc))\n",
    "\n",
    "                \n",
    "            # Check accuracy on validation \n",
    "            if j % 750 == 0:\n",
    "                \n",
    "                # Accumulate loss and accuracy\n",
    "                val_loss_arr, val_acc_arr = [], []\n",
    "                \n",
    "                # Iterate over validation mini-batches\n",
    "                for k in range(num_iterations_validation):\n",
    "                    start_index_val = k * val_batch_size\n",
    "                    end_index_val = (k + 1) * val_batch_size\n",
    "                    \n",
    "                    val_loss, val_acc = sess.run([loss, accuracy], feed_dict={\n",
    "                                            input_sequence: val_source_input[start_index_val:end_index_val],\n",
    "                                            output_sequence: val_target_input[start_index_val:end_index_val, :-1],\n",
    "                                            target_labels: val_target_input[start_index_val:end_index_val, 1:],\n",
    "                                            input_keep_prob: 1.0,\n",
    "                                            output_keep_prob: 1.0})\n",
    "                    \n",
    "                    val_loss_arr.append(val_loss)\n",
    "                    val_acc_arr.append(val_acc)\n",
    "\n",
    "                val_acc = np.mean(val_acc_arr)\n",
    "\n",
    "                # Save model if validation accuracy better\n",
    "                if val_acc > max_val_acc:\n",
    "                    max_val_acc = val_acc\n",
    "                    print(\"VALIDATION loss: \" + str(np.mean(val_loss_arr)) + \", accuracy: \" + str(val_acc))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild graph quickly if want to run only this part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: [None, 3]\n",
      "Encoder embedding: [None, 3, 64]\n",
      "Encoder FW last_state: [None, 128]\n",
      "Encoder BW last_state: [None, 128]\n",
      "Decoder output: [None, None, 256]\n",
      "Logits: [None, None, 90]\n"
     ]
    }
   ],
   "source": [
    "# Parameters network\n",
    "#embedding_size = 300\n",
    "#lstm_neurons = 256\n",
    "\n",
    "# Load dictionaries from pickle\n",
    "english_dictionary = text_processing.load_dump(\"./dumps/eng_dict.pickle\")\n",
    "german_dictionary = text_processing.load_dump(\"./dumps/ger_dict.pickle\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "input_sequence = tf.placeholder(tf.int32, (None, english_dictionary.max_length_sentence), 'inputs')\n",
    "output_sequence = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "target_labels = tf.placeholder(tf.int32, (None), 'targets')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "# Create graph for the network\n",
    "logits = neural_network.create_network(input_sequence, \n",
    "                                       output_sequence, \n",
    "                                       input_keep_prob,\n",
    "                                       output_keep_prob,\n",
    "                                       len(english_dictionary.index_to_word), \n",
    "                                       len(german_dictionary.index_to_word), \n",
    "                                       embedding_size,\n",
    "                                       lstm_hidden_units,\n",
    "                                       depth_lstm_bidirectional_layers,\n",
    "                                       verbose=1)\n",
    "# Predictions\n",
    "scores = tf.nn.softmax(logits)\n",
    "#max_score = tf.reduce_max(scores, axis=1)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt\n",
      "[0 0 6]\n",
      "<PAD> <PAD> hi => <START> hallo <END>\n"
     ]
    }
   ],
   "source": [
    "# TF variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\") \n",
    "\n",
    "    test_source_sentence = [\"HI\"]\n",
    "\n",
    "    for source_sentence in test_source_sentence:\n",
    "\n",
    "        # Normalize & tokenize (cut if longer than max_length_source)  \n",
    "        source_preprocessed = text_processing.preprocess_sentence(source_sentence)[:english_dictionary.max_length_sentence]\n",
    "       \n",
    "        # Convert to numbers\n",
    "        source_encoded = english_dictionary.text_to_indices(source_preprocessed)\n",
    "        \n",
    "        # Add padding\n",
    "        source_input = text_processing.pad_sentence(source_encoded, english_dictionary.max_length_sentence)\n",
    "        print(source_input)\n",
    "        \n",
    "        # Starting target sentence in German\n",
    "        target_sentence = [[\"<START>\"]]\n",
    "        target_encoded = german_dictionary.text_to_indices(target_sentence[0])\n",
    "        \n",
    "        i = 0\n",
    "        word_predicted = 0\n",
    "        while word_predicted != 2: # If <END> (index 2), stop\n",
    "            # Perform prediction\n",
    "            pred = sess.run(predictions, feed_dict={input_sequence: [source_input], \n",
    "                                                    output_sequence: [target_encoded],\n",
    "                                                    input_keep_prob: 1.0,\n",
    "                                                    output_keep_prob: 1.0 })\n",
    "            # Accumulate\n",
    "            target_encoded.append(pred[0][i])\n",
    "            word_predicted = pred[0][i]\n",
    "            i += 1\n",
    "\n",
    "        print(english_dictionary.indices_to_text(source_input) + \" => \"\n",
    "              + german_dictionary.indices_to_text(target_encoded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
