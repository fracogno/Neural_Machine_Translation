{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input is a sentence (sequence) in English \n",
    "- Output is the corresponding sequence in German\n",
    "- Encoder Decoder models with a Deep Bidirectional LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "1. Implement attention\n",
    "2. Gradient clipping\n",
    "2. Comment CODE!!\n",
    "4. GRU CELL\n",
    "\n",
    "TRAINING with BIGGER DATASET (so far trained only with 60 sentence and it was working)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/francesco/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import src.text_processing as text_processing\n",
    "import src.dictionary as dictionary\n",
    "import src.neural_network as neural_network\n",
    "\n",
    "# Update python files\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500,)\n",
      "['i', 'know']\n",
      "['ich', 'wei√ü']\n"
     ]
    }
   ],
   "source": [
    "# Read file containing english and german translations\n",
    "data = text_processing.load_doc(\"./dataset/ENG_to_GER.txt\")\n",
    "\n",
    "# Split data into english and german\n",
    "english_sentences, german_sentences = text_processing.prepare_data(data)\n",
    "\n",
    "# Check and print number of sentences from one language to the other\n",
    "assert(len(english_sentences) == len(german_sentences))\n",
    "print(english_sentences.shape)\n",
    "\n",
    "# Example of sentence with translation\n",
    "print(english_sentences[18])\n",
    "print(german_sentences[18])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split dataset (training + validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 400\n",
      "Validation samples: 100\n"
     ]
    }
   ],
   "source": [
    "# Split percentage of training and validation\n",
    "split_percentage = 0.8\n",
    "\n",
    "# Count how many samples into training dataset\n",
    "total_dataset = len(english_sentences)\n",
    "train_dataset = int(total_dataset * split_percentage)\n",
    "\n",
    "# Set random seed to have always same training and validation split\n",
    "np.random.seed(42)\n",
    "train_indices = np.random.choice(total_dataset, train_dataset, replace=False)\n",
    "\n",
    "# Get training data for the two languages\n",
    "training_english = english_sentences[train_indices]\n",
    "training_german = german_sentences[train_indices]\n",
    "\n",
    "# Get validation data\n",
    "validation_english = np.delete(english_sentences, train_indices)\n",
    "validation_german = np.delete(german_sentences, train_indices)\n",
    "\n",
    "print(\"Training samples: \" + str(training_english.shape[0]))\n",
    "print(\"Validation samples: \" + str(validation_english.shape[0]))\n",
    "\n",
    "# Reset seed for randomness\n",
    "np.random.seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dictionaries for the two languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest sentence in English has 4 tokens.\n",
      "Longest sentence in German has 9 tokens.\n",
      "\n",
      "English dictionary size: 325\n",
      "German dictionary size: 403\n"
     ]
    }
   ],
   "source": [
    "# Calculate longest sentence in the two languages\n",
    "english_max_length = text_processing.max_length_sentence(training_english)\n",
    "german_max_length = text_processing.max_length_sentence(training_german) + 2  # + 2 because of <START> and <END> the beginning\n",
    "\n",
    "print(\"Longest sentence in English has \" + str(english_max_length) + \" tokens.\")\n",
    "print(\"Longest sentence in German has \" + str(german_max_length) + \" tokens.\")\n",
    "print()\n",
    "\n",
    "# Create dictionaries\n",
    "english_dictionary = dictionary.LanguageDictionary(training_english, english_max_length)\n",
    "german_dictionary = dictionary.LanguageDictionary(training_german, german_max_length)\n",
    "\n",
    "# Calculate size of the dictionaries\n",
    "english_dictionary_size = len(english_dictionary.index_to_word)\n",
    "german_dictionary_size = len(german_dictionary.index_to_word)\n",
    "\n",
    "print(\"English dictionary size: \" + str(english_dictionary_size))\n",
    "print(\"German dictionary size: \" + str(german_dictionary_size))\n",
    "\n",
    "# Save dictionaries\n",
    "text_processing.save_dump(english_dictionary, \"./dumps/eng_dict.pickle\")\n",
    "text_processing.save_dump(german_dictionary, \"./dumps/ger_dict.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sequences for the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples : 400\n",
      "(400, 4)\n",
      "(400, 9)\n",
      "Validation samples : 100\n",
      "(100, 4)\n",
      "(100, 9)\n"
     ]
    }
   ],
   "source": [
    "# Prepare sequences of training data\n",
    "train_source_input, train_target_input = text_processing.prepare_sequences(training_english, \n",
    "                                                                       training_german, \n",
    "                                                                       english_dictionary, \n",
    "                                                                       german_dictionary)\n",
    "\n",
    "# Prepare sequences of validation data\n",
    "val_source_input, val_target_input = text_processing.prepare_sequences(validation_english, \n",
    "                                                                       validation_german, \n",
    "                                                                       english_dictionary, \n",
    "                                                                       german_dictionary)\n",
    "\n",
    "# Check if same number of samples\n",
    "assert(len(train_source_input) == len(train_target_input))\n",
    "assert(len(val_source_input) == len(val_target_input))\n",
    "\n",
    "# Print shapes data\n",
    "print(\"Training samples : \" + str(len(train_source_input)))\n",
    "print(train_source_input.shape)\n",
    "print(train_target_input.shape)\n",
    "\n",
    "print(\"Validation samples : \" + str(len(val_source_input)))\n",
    "print(val_source_input.shape)\n",
    "print(val_target_input.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print sample input data in English, German and next word to be predicted in German"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 13 21 22]\n",
      "[ 1 22 15 16 18  2  0  0  0]\n",
      "SOURCE => <PAD> i am well\n",
      "TARGET => <START> mir geht es gut <END> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "sample_sentence_index = 7\n",
    "print(train_source_input[sample_sentence_index])\n",
    "print(train_target_input[sample_sentence_index])\n",
    "\n",
    "print(\"SOURCE => \" + english_dictionary.indices_to_text(train_source_input[sample_sentence_index]))\n",
    "print(\"TARGET => \" + german_dictionary.indices_to_text(train_target_input[sample_sentence_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 150\n",
    "batch_size = 128\n",
    "embedding_size = 256\n",
    "lstm_hidden_units = 64\n",
    "lr = 1e-3\n",
    "depth_lstm_bidirectional_layers = 2\n",
    "keep_dropout_prob = 0.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Seq2seq neural network graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder/concat:0\", shape=(?, 4, 128), dtype=float32)\n",
      "Tensor(\"decoder/ExpandDims:0\", shape=(?, 8, 1, 128), dtype=float32)\n",
      "Tensor(\"decoder/dense/BiasAdd:0\", shape=(?, 8, 4, 32), dtype=float32)\n",
      "Tensor(\"decoder/dense_1/BiasAdd:0\", shape=(?, 8, 1, 32), dtype=float32)\n",
      "Tensor(\"decoder/Tanh:0\", shape=(?, 8, 4, 32), dtype=float32)\n",
      "Tensor(\"decoder/dense_2/BiasAdd:0\", shape=(?, 8, 4, 1), dtype=float32)\n",
      "Tensor(\"decoder/transpose_1:0\", shape=(?, 8, 4, 1), dtype=float32)\n",
      "Tensor(\"decoder/Sum:0\", shape=(?, 8, 128), dtype=float32)\n",
      "Tensor(\"decoder/embedding_lookup/Identity:0\", shape=(?, ?, 256), dtype=float32)\n",
      "Tensor(\"decoder/concat_2:0\", shape=(?, 8, 384), dtype=float32)\n",
      "Tensor(\"decoder/rnn/transpose_1:0\", shape=(?, 8, 128), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 8, 403), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "input_sequence = tf.placeholder(tf.int32, (None, english_dictionary.max_length_sentence), 'inputs')\n",
    "output_sequence = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "target_labels = tf.placeholder(tf.int32, (None, None), 'targets')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "decoder_outputs_tensor = tf.placeholder(tf.float32, (None, german_dictionary.max_length_sentence-1, \n",
    "                                                     lstm_hidden_units * 2), 'output')\n",
    "# Create graph for the network\n",
    "logits, dec_output = neural_network.create_network(input_sequence, \n",
    "                                                   output_sequence, \n",
    "                                                   input_keep_prob,\n",
    "                                                   output_keep_prob,\n",
    "                                                   decoder_outputs_tensor,\n",
    "                                                   english_dictionary_size, \n",
    "                                                   german_dictionary_size, \n",
    "                                                   embedding_size,\n",
    "                                                   lstm_hidden_units,\n",
    "                                                   depth_lstm_bidirectional_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set the loss function, optimizer and other useful tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross entropy loss after softmax of logits\n",
    "ce = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=target_labels)\n",
    "loss = tf.reduce_mean(ce)\n",
    "\n",
    "# Using Adam (Adaptive learning rate + momentum) for the update of the weights of the network\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr) #.minimize(loss)\n",
    "gradients, variables = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 5.0)\n",
    "optimize = optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "# Useful tensors\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=2))\n",
    "correct_mask = tf.to_float(tf.equal(predictions, target_labels))\n",
    "accuracy = tf.reduce_mean(tf.reduce_mean(tf.cast(correct_mask, tf.float32), axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_decoder_outputs(batch_size, target_length, hidden_size, source, target_in):\n",
    "    \n",
    "    # Feature multiply by two because of bidirectional lstm\n",
    "    first_output = np.zeros((batch_size, target_length, 2 * hidden_size))\n",
    "    for i in range(target_length-1):\n",
    "\n",
    "        fgg = sess.run(dec_output, feed_dict={\n",
    "            input_sequence: source,\n",
    "            output_sequence: target_in,\n",
    "            decoder_outputs_tensor: first_output,\n",
    "            input_keep_prob: keep_dropout_prob,\n",
    "            output_keep_prob: keep_dropout_prob,\n",
    "        })\n",
    "        first_output[:,i+1] = fgg[:,i]\n",
    "        \n",
    "    return first_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training iterations per epoch: 3\n",
      "Loss: 6.037218, Accuracy: 0.001953125\n",
      "VALIDATION loss: 5.704887, accuracy: 0.38875\n",
      "Loss: 5.7036953, Accuracy: 0.30273438\n",
      "VALIDATION loss: 5.386392, accuracy: 0.51125\n",
      "Loss: 5.338003, Accuracy: 0.5503472\n",
      "VALIDATION loss: 5.06067, accuracy: 0.5125\n",
      "Training epoch: 1, AVG loss: 5.692972, AVG accuracy: 0.28501156\n",
      "\n",
      "Loss: 5.0241504, Accuracy: 0.5361328\n",
      "Loss: 4.614168, Accuracy: 0.5498047\n",
      "Loss: 4.282479, Accuracy: 0.5486111\n",
      "Training epoch: 2, AVG loss: 4.640266, AVG accuracy: 0.5448496\n",
      "\n",
      "Loss: 3.9157202, Accuracy: 0.5449219\n",
      "Loss: 3.659391, Accuracy: 0.53808594\n",
      "Loss: 3.2987156, Accuracy: 0.5503472\n",
      "Training epoch: 3, AVG loss: 3.624609, AVG accuracy: 0.54445165\n",
      "\n",
      "Loss: 3.1242027, Accuracy: 0.5390625\n",
      "Loss: 2.8814511, Accuracy: 0.5576172\n",
      "Loss: 2.8445296, Accuracy: 0.5399306\n",
      "Training epoch: 4, AVG loss: 2.950061, AVG accuracy: 0.54553676\n",
      "\n",
      "Loss: 2.743403, Accuracy: 0.54296875\n",
      "Loss: 2.6450417, Accuracy: 0.5546875\n",
      "Loss: 2.6968482, Accuracy: 0.5381944\n",
      "VALIDATION loss: 2.8977878, accuracy: 0.51375\n",
      "Training epoch: 5, AVG loss: 2.6950977, AVG accuracy: 0.54528356\n",
      "\n",
      "Loss: 2.6651042, Accuracy: 0.53515625\n",
      "Loss: 2.4988475, Accuracy: 0.5517578\n",
      "Loss: 2.4810176, Accuracy: 0.5486111\n",
      "VALIDATION loss: 2.7253528, accuracy: 0.515\n",
      "Training epoch: 6, AVG loss: 2.5483232, AVG accuracy: 0.5451751\n",
      "\n",
      "Loss: 2.4110494, Accuracy: 0.54589844\n",
      "Loss: 2.3002853, Accuracy: 0.5498047\n",
      "Loss: 2.320134, Accuracy: 0.5425347\n",
      "VALIDATION loss: 2.5133889, accuracy: 0.52875\n",
      "Training epoch: 7, AVG loss: 2.3438227, AVG accuracy: 0.5460793\n",
      "\n",
      "Loss: 2.215469, Accuracy: 0.5517578\n",
      "VALIDATION loss: 2.4557104, accuracy: 0.54625\n",
      "Loss: 2.1471364, Accuracy: 0.56933594\n",
      "VALIDATION loss: 2.4084466, accuracy: 0.5525\n",
      "Loss: 2.1256316, Accuracy: 0.5763889\n",
      "VALIDATION loss: 2.371261, accuracy: 0.5675\n",
      "Training epoch: 8, AVG loss: 2.1627455, AVG accuracy: 0.56582755\n",
      "\n",
      "Loss: 2.1752584, Accuracy: 0.5673828\n",
      "VALIDATION loss: 2.3426642, accuracy: 0.6275\n",
      "Loss: 1.9743268, Accuracy: 0.67871094\n",
      "VALIDATION loss: 2.319804, accuracy: 0.665\n",
      "Loss: 2.0091243, Accuracy: 0.6875\n",
      "VALIDATION loss: 2.3005774, accuracy: 0.6725\n",
      "Training epoch: 9, AVG loss: 2.0529032, AVG accuracy: 0.64453125\n",
      "\n",
      "Loss: 1.9624674, Accuracy: 0.7011719\n",
      "VALIDATION loss: 2.2831728, accuracy: 0.67375\n",
      "Loss: 2.0187438, Accuracy: 0.68847656\n",
      "Loss: 1.9835854, Accuracy: 0.6944444\n",
      "Training epoch: 10, AVG loss: 1.9882655, AVG accuracy: 0.6946976\n",
      "\n",
      "Loss: 1.9195673, Accuracy: 0.6982422\n",
      "Loss: 1.8992258, Accuracy: 0.69433594\n",
      "Loss: 1.9628065, Accuracy: 0.6935764\n",
      "Training epoch: 11, AVG loss: 1.9272, AVG accuracy: 0.6953848\n",
      "\n",
      "Loss: 1.9036349, Accuracy: 0.69140625\n",
      "Loss: 1.9370956, Accuracy: 0.69140625\n",
      "Loss: 1.7976557, Accuracy: 0.7022569\n",
      "Training epoch: 12, AVG loss: 1.8794621, AVG accuracy: 0.6950231\n",
      "\n",
      "Loss: 1.8495188, Accuracy: 0.69140625\n",
      "Loss: 1.8761511, Accuracy: 0.6904297\n",
      "Loss: 1.7829149, Accuracy: 0.703125\n",
      "Training epoch: 13, AVG loss: 1.836195, AVG accuracy: 0.694987\n",
      "\n",
      "Loss: 1.8416932, Accuracy: 0.6855469\n",
      "Loss: 1.8080809, Accuracy: 0.6972656\n",
      "Loss: 1.757562, Accuracy: 0.7013889\n",
      "Training epoch: 14, AVG loss: 1.8024454, AVG accuracy: 0.6947338\n",
      "\n",
      "Loss: 1.8200867, Accuracy: 0.68847656\n",
      "Loss: 1.747129, Accuracy: 0.6953125\n",
      "Loss: 1.7378033, Accuracy: 0.703125\n",
      "Training epoch: 15, AVG loss: 1.7683396, AVG accuracy: 0.695638\n",
      "\n",
      "Loss: 1.7800663, Accuracy: 0.6855469\n",
      "Loss: 1.7545589, Accuracy: 0.703125\n",
      "VALIDATION loss: 2.0682964, accuracy: 0.68\n",
      "Loss: 1.6882386, Accuracy: 0.6996528\n",
      "VALIDATION loss: 2.0612123, accuracy: 0.6825\n",
      "Training epoch: 16, AVG loss: 1.7409545, AVG accuracy: 0.69610816\n",
      "\n",
      "Loss: 1.7681987, Accuracy: 0.69921875\n",
      "Loss: 1.7100531, Accuracy: 0.7050781\n",
      "Loss: 1.6573592, Accuracy: 0.7057292\n",
      "VALIDATION loss: 2.0423427, accuracy: 0.685\n",
      "Training epoch: 17, AVG loss: 1.7118703, AVG accuracy: 0.703342\n",
      "\n",
      "Loss: 1.711035, Accuracy: 0.7050781\n",
      "VALIDATION loss: 2.036895, accuracy: 0.68625\n",
      "Loss: 1.6105564, Accuracy: 0.7167969\n",
      "Loss: 1.7174639, Accuracy: 0.703125\n",
      "VALIDATION loss: 2.0277376, accuracy: 0.68875\n",
      "Training epoch: 18, AVG loss: 1.6796851, AVG accuracy: 0.7083333\n",
      "\n",
      "Loss: 1.6156204, Accuracy: 0.7207031\n",
      "VALIDATION loss: 2.023216, accuracy: 0.69375\n",
      "Loss: 1.73508, Accuracy: 0.7060547\n",
      "Loss: 1.6111681, Accuracy: 0.7256944\n",
      "VALIDATION loss: 2.0141463, accuracy: 0.695\n",
      "Training epoch: 19, AVG loss: 1.653956, AVG accuracy: 0.71748406\n",
      "\n",
      "Loss: 1.6757455, Accuracy: 0.71972656\n",
      "VALIDATION loss: 2.009539, accuracy: 0.70125\n"
     ]
    }
   ],
   "source": [
    "# Training data variables\n",
    "iterations_training = max((len(train_source_input) // batch_size), 1)\n",
    "print(\"Training iterations per epoch: \" + str(iterations_training))\n",
    "training_overfit = False\n",
    "consecutive_validation_without_saving = 0\n",
    "\n",
    "# Validation data variables\n",
    "max_val_acc = 0\n",
    "iterations_validation = max((len(val_source_input) // batch_size), 1)\n",
    "\n",
    "# Before each epoch, shuffle training dataset\n",
    "indices = list(range(len(train_source_input)))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    # Initialize variables in the graph\n",
    "    saver = tf.train.Saver()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Iterate over epochs\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Shuffle data (with random seed for debug) to not train the network always with the same order\n",
    "        np.random.seed(42)\n",
    "        np.random.shuffle(indices)\n",
    "        train_source_input = train_source_input[indices]\n",
    "        train_target_input = train_target_input[indices]\n",
    "        \n",
    "        # Vector accumulating accuracy and loss during for one epoch\n",
    "        total_accuracies, total_losses = [], []\n",
    "\n",
    "        # Iterate over mini-batches\n",
    "        for j in range(iterations_training):\n",
    "            start_index = j * batch_size\n",
    "            end_index = (j + 1) * batch_size \n",
    "            \n",
    "            # If last batch, take also elements that are less than batch_size\n",
    "            if j == (iterations_training - 1):\n",
    "                end_index += (batch_size - 1)\n",
    "                \n",
    "            dec_out_tmp = get_decoder_outputs(len(train_source_input[start_index:end_index]), \n",
    "                                              german_dictionary.max_length_sentence - 1, \n",
    "                                              lstm_hidden_units,\n",
    "                                              train_source_input[start_index:end_index],\n",
    "                                              train_target_input[start_index:end_index, :-1])\n",
    "\n",
    "            _, avg_accuracy, avg_loss = sess.run([optimizer, accuracy, loss], feed_dict={\n",
    "                                                input_sequence: train_source_input[start_index:end_index],\n",
    "                                                output_sequence: train_target_input[start_index:end_index, :-1],\n",
    "                                                target_labels: train_target_input[start_index:end_index, 1:],\n",
    "                                                input_keep_prob: keep_dropout_prob,\n",
    "                                                output_keep_prob: keep_dropout_prob,\n",
    "                                                decoder_outputs_tensor: dec_out_tmp })\n",
    "            \n",
    "            # Add values for this mini-batch iterations\n",
    "            total_losses.append(avg_loss) \n",
    "            total_accuracies.append(avg_accuracy)\n",
    "\n",
    "            # Print loss and accuracy\n",
    "            if (j+1) % 1 == 0:\n",
    "                print(\"Loss: \" + str(avg_loss) + \", Accuracy: \" + str(avg_accuracy))\n",
    "\n",
    "            # Statistics on validation set\n",
    "            if (j+1) % 1 == 0:\n",
    "                \n",
    "                # Accumulate validation statistics\n",
    "                val_accuracies, val_losses = [], []\n",
    "\n",
    "                # Iterate over mini-batches\n",
    "                for k in range(iterations_validation):\n",
    "                    start_index = k * batch_size\n",
    "                    end_index = (k + 1) * batch_size \n",
    "                    \n",
    "                    if j == (iterations_validation - 1):\n",
    "                        end_index += (batch_size - 1)\n",
    "                    \n",
    "                    dec_out_tmp = get_decoder_outputs(len(val_source_input[start_index:end_index]), \n",
    "                                                      german_dictionary.max_length_sentence - 1, \n",
    "                                                      lstm_hidden_units,\n",
    "                                                      val_source_input[start_index:end_index],\n",
    "                                                      val_target_input[start_index:end_index, :-1])\n",
    "                    \n",
    "                    avg_accuracy, avg_loss = sess.run([accuracy, loss], feed_dict={\n",
    "                                            input_sequence: val_source_input[start_index:end_index],\n",
    "                                            output_sequence: val_target_input[start_index:end_index, :-1],\n",
    "                                            target_labels: val_target_input[start_index:end_index, 1:],\n",
    "                                            input_keep_prob: 1.0,\n",
    "                                            output_keep_prob: 1.0,\n",
    "                                            decoder_outputs_tensor: dec_out_tmp })                    \n",
    "                    \n",
    "                    # Statistics over the mini-batch\n",
    "                    val_losses.append(avg_loss) \n",
    "                    val_accuracies.append(avg_accuracy)\n",
    "            \n",
    "                # Average validation accuracy over batches\n",
    "                final_val_accuracy = np.mean(val_accuracies)\n",
    "                \n",
    "                # Save model if validation accuracy better\n",
    "                if final_val_accuracy > max_val_acc:\n",
    "                    consecutive_validation_without_saving = 0\n",
    "                    max_val_acc = final_val_accuracy\n",
    "                    print(\"VALIDATION loss: \" + str(np.mean(val_losses)) + \", accuracy: \" + str(final_val_accuracy))\n",
    "                    save_path = saver.save(sess, \"./checkpoints/model.ckpt\")\n",
    "                else:\n",
    "                    # Count every time check validation accuracy\n",
    "                    consecutive_validation_without_saving += 1\n",
    "                \n",
    "                # If checked validation time many consecutive times without having improvement in accuracy\n",
    "                if consecutive_validation_without_saving >= 30:\n",
    "                    #training_overfit = True\n",
    "                    print(1)\n",
    "        # Epoch statistics\n",
    "        print(\"Training epoch: \" + str(i+1) + \", AVG loss: \" + str(np.mean(np.array(total_losses))) + \n",
    "              \", AVG accuracy: \" + str(np.mean(np.array(total_accuracies))) + \"\\n\")\n",
    "        \n",
    "        if training_overfit:\n",
    "            print(\"Early stopping training because it starts overfitting\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rebuild graph quickly if want to run only this part of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"encoder/concat:0\", shape=(?, 4, 128), dtype=float32)\n",
      "Tensor(\"decoder/ExpandDims:0\", shape=(?, 8, 1, 128), dtype=float32)\n",
      "Tensor(\"decoder/dense/BiasAdd:0\", shape=(?, 8, 4, 32), dtype=float32)\n",
      "Tensor(\"decoder/dense_1/BiasAdd:0\", shape=(?, 8, 1, 32), dtype=float32)\n",
      "Tensor(\"decoder/Tanh:0\", shape=(?, 8, 4, 32), dtype=float32)\n",
      "Tensor(\"decoder/dense_2/BiasAdd:0\", shape=(?, 8, 4, 1), dtype=float32)\n",
      "Tensor(\"decoder/transpose_1:0\", shape=(?, 8, 4, 1), dtype=float32)\n",
      "Tensor(\"decoder/Sum:0\", shape=(?, 8, 128), dtype=float32)\n",
      "Tensor(\"decoder/embedding_lookup/Identity:0\", shape=(?, ?, 256), dtype=float32)\n",
      "Tensor(\"decoder/concat_2:0\", shape=(?, 8, 384), dtype=float32)\n",
      "Tensor(\"decoder/rnn/transpose_1:0\", shape=(?, 8, 128), dtype=float32)\n",
      "Tensor(\"dense/BiasAdd:0\", shape=(?, 8, 403), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Load dictionaries from pickle\n",
    "english_dictionary = text_processing.load_dump(\"./dumps/eng_dict.pickle\")\n",
    "german_dictionary = text_processing.load_dump(\"./dumps/ger_dict.pickle\")\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Placeholders\n",
    "input_sequence = tf.placeholder(tf.int32, (None, english_dictionary.max_length_sentence), 'inputs')\n",
    "output_sequence = tf.placeholder(tf.int32, (None, None), 'output')\n",
    "target_labels = tf.placeholder(tf.int32, (None, None), 'targets')\n",
    "input_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_input')\n",
    "output_keep_prob = tf.placeholder(tf.float32, (None), 'dropout_output')\n",
    "\n",
    "decoder_outputs_tensor = tf.placeholder(tf.float32, (None, german_dictionary.max_length_sentence-1, \n",
    "                                                     lstm_hidden_units * 2), 'output')\n",
    "\n",
    "# Create graph for the network\n",
    "logits, dec_output = neural_network.create_network(input_sequence, \n",
    "                                                   output_sequence, \n",
    "                                                   input_keep_prob,\n",
    "                                                   output_keep_prob,\n",
    "                                                   decoder_outputs_tensor,\n",
    "                                                   len(english_dictionary.index_to_word), \n",
    "                                                   len(german_dictionary.index_to_word), \n",
    "                                                   embedding_size,\n",
    "                                                   lstm_hidden_units,\n",
    "                                                   depth_lstm_bidirectional_layers)\n",
    "# Predictions\n",
    "scores = tf.nn.softmax(logits)\n",
    "predictions = tf.to_int32(tf.argmax(scores, axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt\n",
      "<PAD> <PAD> <PAD> hello => <START> tom <END>\n"
     ]
    }
   ],
   "source": [
    "# TF variables\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, \"./checkpoints/model.ckpt\") \n",
    "\n",
    "    test_source_sentence = [\"hello\"]\n",
    "    #test_source_sentence = validation_english\n",
    "\n",
    "    for source_sentence in test_source_sentence:\n",
    "\n",
    "        # ONLY IF VALIDATION ENGLISH DATASET USED (DEBUG)\n",
    "        #source_sentence = \" \".join(source_sentence)\n",
    "\n",
    "        # Normalize & tokenize (cut if longer than max_length_source)  \n",
    "        source_preprocessed = text_processing.preprocess_sentence(source_sentence)\n",
    "        \n",
    "        # Convert to numbers\n",
    "        source_encoded = english_dictionary.text_to_indices(source_preprocessed)\n",
    "        \n",
    "        # Add padding\n",
    "        source_input = text_processing.pad_sentence(source_encoded, english_dictionary.max_length_sentence)\n",
    "        #print(english_dictionary.indices_to_text(source_input))\n",
    "        \n",
    "        # Starting target sentence in German\n",
    "        target_sentence = [[\"<START>\"]]\n",
    "        target_encoded = german_dictionary.text_to_indices(target_sentence[0])\n",
    "\n",
    "        i = 0\n",
    "        word_predicted = 0\n",
    "        while word_predicted != 2: # If <END> (index 2), stop\n",
    "            \n",
    "            target_encoded_pad = text_processing.pad_sentence(target_encoded, \n",
    "                                                          german_dictionary.max_length_sentence - 1, \n",
    "                                                           pad_before=False)\n",
    "\n",
    "            dec_out_tmp = get_decoder_outputs(1, \n",
    "                                              german_dictionary.max_length_sentence - 1, \n",
    "                                              lstm_hidden_units,\n",
    "                                              [source_input],\n",
    "                                              [target_encoded_pad])\n",
    "        \n",
    "            # Perform prediction\n",
    "            pred = sess.run(predictions, feed_dict={input_sequence: [source_input], \n",
    "                                                    output_sequence: [target_encoded_pad],\n",
    "                                                    input_keep_prob: 1.0,\n",
    "                                                    output_keep_prob: 1.0,\n",
    "                                                    decoder_outputs_tensor: dec_out_tmp })\n",
    "            \n",
    "            # Accumulate\n",
    "            target_encoded.append(pred[0][i])\n",
    "            word_predicted = pred[0][i]\n",
    "            \n",
    "            if i > german_dictionary.max_length_sentence:\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "        print(english_dictionary.indices_to_text(source_input) + \" => \"\n",
    "              + german_dictionary.indices_to_text(target_encoded))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
